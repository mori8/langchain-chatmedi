{"downloads": 12, "id": "disi-unibo-nlp/MedGENIE-fid-flan-t5-base-medmcqa", "likes": 0, "pipeline_tag": "question-answering", "task": "question-answering-about-medical-domain", "meta": {"language": "en", "tags": ["medical", "generated_from_trainer"], "widget": [{"text": "Q: Which of the following is not true for myelinated nerve fibers: A. Impulse through myelinated fibers is slower than non-myelinated fibers B. Membrane currents are generated at nodes of Ranvier C. Saltatory conduction of impulses is seen D. Local anesthesia is effective only when the nerve is not covered by myelin sheath, A: The myelin sheath of myelinated nerve fibers is a covering that acts as insulation and increases the rate of conduction. Therefore, impulse through myelinated fibers is faster than non-myelinated fibers. Understanding these differences in structure and function between these two types of nerve cells helps us appreciate how local anesthetics work, as well as why they are more effective on small diameter axons (which are not heavily myelinated)."}]}, "description": "MedGENIE comprises a collection of language models designed to utilize generated contexts, rather than retrieved ones, for addressing multiple-choice open-domain questions in the medical field. Specifically, MedGENIE-fid-flan-t5-base-medmcqa is a fusion-in-decoder (FID) model based on flan-t5-base, trained on the MedMCQA dataset and grounded on artificial contexts generated by PMC-LLaMA-13B. This model achieves performance levels comparable to state-of-the-art (SOTA) larger models on both MedMCQA and MMLU-Medical benchmarks.", "endpoint": "https://api-inference.huggingface.co/models/disi-unibo-nlp/MedGENIE-fid-flan-t5-base-medmcqa"}
{"downloads": 26, "id": "exafluence/BERT-ClinicalQA", "likes": 0, "pipeline_tag": "question-answering", "task": "question-answering-about-medical-domain", "meta": {"language": "en", "tags": ["medical"], "widget": [{"text": ""}]}, "description": "no model card", "endpoint": "https://api-inference.huggingface.co/models/exafluence/BERT-ClinicalQA"}
{"downloads": 20, "id": "Shushant/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext-ContaminationQAmodel_PubmedBERT", "likes": 1, "pipeline_tag": "question-answering", "task": "question-answering-about-medical-domain", "meta": {"language": "en", "tags": ["medical"], "widget": [{"text": ""}]}, "description": "This model is a fine-tuned version of [microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext](https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext) on an unknown dataset.It achieves the following results on the evaluation set:- Loss: 2.7515", "endpoint": "https://api-inference.huggingface.co/models/Shushant/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext-ContaminationQAmodel_PubmedBERT"}
{"downloads": 166, "id": "sultan/BioM-ELECTRA-Large-SQuAD2", "likes": 10, "pipeline_tag": "question-answering", "task": "question-answering-about-medical-domain", "meta": {"language": "en", "tags": [""], "widget": [{"text": ""}]}, "description": "The impact of design choices on the performance of biomedical language models recently has been a subject for investigation. In this paper, we empirically study biomedical domain adaptation with large transformer models using different design choices. We evaluate the performance of our pretrained models against other existing biomedical language models in the literature. Our results show that we achieve state-of-the-art results on several biomedical domain tasks despite using similar or less computational cost compared to other models in the literature. Our findings highlight the significant effect of design choices on improving the performance of biomedical language models. We fine-tuned BioM-ELECTRA-Large, which was pre-trained on PubMed Abstracts, on the SQuAD2.0 dataset. Fine-tuning the biomedical language model on the SQuAD dataset helps improve the score on the BioASQ challenge. If you plan to work with BioASQ or biomedical QA tasks, it's better to use this model over BioM-ELECTRA-Large. This model (TensorFlow version ) took the lead in the BioASQ", "endpoint": "https://api-inference.huggingface.co/models/sultan/BioM-ELECTRA-Large-SQuAD2"}
{"downloads": 6, "id": "dineshcr7/Type_MediVQA", "likes": 0, "pipeline_tag": "visual-question-answering", "task": "visual-question-answering-about-medical-domain", "meta": {"language": "en", "tags": [""], "widget": [{"text": "Medical Visual Question Answering"}]}, "description": "", "endpoint": "https://api-inference.huggingface.co/models/dineshcr7/Type_MediVQA"}
{"downloads": 143, "id": "danyalmalik/stable-diffusion-chest-xray", "likes": 3, "pipeline_tag": "text-to-image", "task": "text-to-image", "meta": {"language": "en", "tags": [""], "widget": [{"text": ""}]}, "description": "Chest-Xray-Fine-Tuning---trained-on-chest-xray14-dataset-(only-1k-images-as-of-now) Dreambooth model trained by danyalmalik with TheLastBen's fast-DreamBooth notebook", "endpoint": "https://api-inference.huggingface.co/models/danyalmalik/stable-diffusion-chest-xray"}
{"downloads": 21013, "id": "BioMistral/BioMistral-7B", "likes": 329, "pipeline_tag": "text-generation", "task": "medical-text-generation", "meta": {"language": "en", "tags": ["medical", "biology"], "widget": [{"text": ""}]}, "description": "**BioMistral** is a suite of Mistral-based further pre-trained open source models suited for the medical domains and pre-trained using textual data from PubMed Central Open Access (CC0, CC BY, CC BY-SA, and CC BY-ND). All the models are trained using the CNRS (French National Centre for Scientific Research) [Jean Zay](http://www.idris.fr/jean-zay/) French HPC.", "endpoint": "https://kyiu7f36yx8hombe.us-east-1.aws.endpoints.huggingface.cloud"}
{"downloads": 141, "id": "as-cle-bert/segformer-v1-breastcancer", "likes": 0, "pipeline_tag": "image-segmentation", "task": "medical-image-segmentation", "meta": {"language": "en", "tags": ["generated_from_trainer"], "widget": [{"text": ""}]}, "description": "", "endpoint": "https://api-inference.huggingface.co/models/as-cle-bert/segformer-v1-breastcancer"}